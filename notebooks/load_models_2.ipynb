{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54574494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartek/Studia/ZZSN/projekt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets\n",
    "import torch.nn as nn \n",
    "import torch\n",
    "import torchvision.transforms as trn\n",
    "from pathlib import Path\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8d423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform):\n",
    "        super().__init__(root, transform)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return sample, target, index\n",
    "\n",
    "class ImageNet100:\n",
    "    def __init__(self,\n",
    "                 preprocess,\n",
    "                 location=os.path.expanduser('../data_new'),\n",
    "                 batch_size=32,\n",
    "                 num_workers=16):\n",
    "        # Data loading code\n",
    "        location = '../data_new'\n",
    "        traindir = os.path.join(location, 'ImageNet100', 'train')\n",
    "        # valdir = os.path.join(location, 'ImageNet100', 'val')\n",
    "\n",
    "        self.train_dataset = ImageFolderDataset(\n",
    "            traindir, transform=preprocess)\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        # self.test_dataset = ImageFolderDataset(valdir, transform=preprocess)\n",
    "        # self.test_loader = torch.utils.data.DataLoader(\n",
    "        #     self.test_dataset,\n",
    "        #     batch_size=batch_size,\n",
    "        #     num_workers=num_workers\n",
    "        # )\n",
    "        # self.test_loader_shuffle = torch.utils.data.DataLoader(\n",
    "        #     self.test_dataset,\n",
    "        #     shuffle=True,\n",
    "        #     batch_size=batch_size,\n",
    "        #     num_workers=num_workers\n",
    "        # )\n",
    "        idx_to_class = dict((v, k)\n",
    "                            for k, v in self.train_dataset.class_to_idx.items())\n",
    "        self.classnames = [idx_to_class[i].replace(\n",
    "            '_', ' ') for i in range(len(idx_to_class))]\n",
    "\n",
    "        labels = {\n",
    "                \"n01968897\": \"chambered nautilus\",\n",
    "                \"n01770081\": \"harvestman\",\n",
    "                \"n01818515\": \"macaw\",\n",
    "                \"n02011460\": \"bittern\",\n",
    "                \"n01496331\": \"electric ray\",\n",
    "                \"n01847000\": \"drake\",\n",
    "                \"n01687978\": \"agama\",\n",
    "                \"n01740131\": \"night snake\",\n",
    "                \"n01537544\": \"indigo bunting\",\n",
    "                \"n01491361\": \"tiger shark\",\n",
    "                \"n02007558\": \"flamingo\",\n",
    "                \"n01735189\": \"garter snake\",\n",
    "                \"n01630670\": \"common newt\",\n",
    "                \"n01440764\": \"tench\",\n",
    "                \"n01819313\": \"sulphur-crested cockatoo\",\n",
    "                \"n02002556\": \"white stork\",\n",
    "                \"n01667778\": \"terrapin\",\n",
    "                \"n01755581\": \"diamondback\",\n",
    "                \"n01924916\": \"flatworm\",\n",
    "                \"n01751748\": \"sea snake\",\n",
    "                \"n01984695\": \"spiny lobster\",\n",
    "                \"n01729977\": \"green snake\",\n",
    "                \"n01614925\": \"bald eagle\",\n",
    "                \"n01608432\": \"kite\",\n",
    "                \"n01443537\": \"goldfish\",\n",
    "                \"n01770393\": \"scorpion\",\n",
    "                \"n01855672\": \"goose\",\n",
    "                \"n01560419\": \"bulbul\",\n",
    "                \"n01592084\": \"chickadee\",\n",
    "                \"n01914609\": \"sea anemone\",\n",
    "                \"n01582220\": \"magpie\",\n",
    "                \"n01667114\": \"mud turtle\",\n",
    "                \"n01985128\": \"crayfish\",\n",
    "                \"n01820546\": \"lorikeet\",\n",
    "                \"n01773797\": \"garden spider\",\n",
    "                \"n02006656\": \"spoonbill\",\n",
    "                \"n01986214\": \"hermit crab\",\n",
    "                \"n01484850\": \"great white shark\",\n",
    "                \"n01749939\": \"green mamba\",\n",
    "                \"n01828970\": \"bee eater\",\n",
    "                \"n02018795\": \"bustard\",\n",
    "                \"n01695060\": \"Komodo dragon\",\n",
    "                \"n01729322\": \"hognose snake\",\n",
    "                \"n01677366\": \"common iguana\",\n",
    "                \"n01734418\": \"king snake\",\n",
    "                \"n01843383\": \"toucan\",\n",
    "                \"n01806143\": \"peacock\",\n",
    "                \"n01773549\": \"barn spider\",\n",
    "                \"n01775062\": \"wolf spider\",\n",
    "                \"n01728572\": \"thunder snake\",\n",
    "                \"n01601694\": \"water ouzel\",\n",
    "                \"n01978287\": \"Dungeness crab\",\n",
    "                \"n01930112\": \"nematode\",\n",
    "                \"n01739381\": \"vine snake\",\n",
    "                \"n01883070\": \"wombat\",\n",
    "                \"n01774384\": \"black widow\",\n",
    "                \"n02037110\": \"oystercatcher\",\n",
    "                \"n01795545\": \"black grouse\",\n",
    "                \"n02027492\": \"red-backed sandpiper\",\n",
    "                \"n01531178\": \"goldfinch\",\n",
    "                \"n01944390\": \"snail\",\n",
    "                \"n01494475\": \"hammerhead\",\n",
    "                \"n01632458\": \"spotted salamander\",\n",
    "                \"n01698640\": \"American alligator\",\n",
    "                \"n01675722\": \"banded gecko\",\n",
    "                \"n01877812\": \"wallaby\",\n",
    "                \"n01622779\": \"great grey owl\",\n",
    "                \"n01910747\": \"jellyfish\",\n",
    "                \"n01860187\": \"black swan\",\n",
    "                \"n01796340\": \"ptarmigan\",\n",
    "                \"n01833805\": \"hummingbird\",\n",
    "                \"n01685808\": \"whiptail\",\n",
    "                \"n01756291\": \"sidewinder\",\n",
    "                \"n01514859\": \"hen\",\n",
    "                \"n01753488\": \"horned viper\",\n",
    "                \"n02058221\": \"albatross\",\n",
    "                \"n01632777\": \"axolotl\",\n",
    "                \"n01644900\": \"tailed frog\",\n",
    "                \"n02018207\": \"American coot\",\n",
    "                \"n01664065\": \"loggerhead\",\n",
    "                \"n02028035\": \"redshank\",\n",
    "                \"n02012849\": \"crane\",\n",
    "                \"n01776313\": \"tick\",\n",
    "                \"n02077923\": \"sea lion\",\n",
    "                \"n01774750\": \"tarantula\",\n",
    "                \"n01742172\": \"boa constrictor\",\n",
    "                \"n01943899\": \"conch\",\n",
    "                \"n01798484\": \"prairie chicken\",\n",
    "                \"n02051845\": \"pelican\",\n",
    "                \"n01824575\": \"coucal\",\n",
    "                \"n02013706\": \"limpkin\",\n",
    "                \"n01955084\": \"chiton\",\n",
    "                \"n01773157\": \"black and gold garden spider\",\n",
    "                \"n01665541\": \"leatherback turtle\",\n",
    "                \"n01498041\": \"stingray\",\n",
    "                \"n01978455\": \"rock crab\",\n",
    "                \"n01693334\": \"green lizard\",\n",
    "                \"n01950731\": \"sea slug\",\n",
    "                \"n01829413\": \"hornbill\",\n",
    "                \"n01514668\": \"cock\"\n",
    "            }\n",
    "        self.classnames = [labels[class_name] for class_name in self.classnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2885ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image classifier from ../models/finetuned_s2_fs1.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m head \u001b[38;5;241m=\u001b[39m ClassificationHead(\n\u001b[1;32m     24\u001b[0m     normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     weights\u001b[38;5;241m=\u001b[39mdummy_weights\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageClassifier(encoder, head)\n\u001b[0;32m---> 28\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/zzsn_backdoor_detection/notebooks/../../BadMerging/src/modeling.py:221\u001b[0m, in \u001b[0;36mImageClassifier.load\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename):\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading image classifier from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mtorch_load(filename)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='../models/finetuned_s2_fs1.pt', force_image_size=224)\n",
    "checkpoint = \"../models/finetuned_s2_fs1.pt\"\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "device = \"cuda\" \n",
    "# model = torch.load(checkpoint, map_location=device)\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32')\n",
    "# image_net = ImageNet100(preprocess)\n",
    "\n",
    "from BadMerging.src.modeling import ClassificationHead, ImageClassifier\n",
    "\n",
    "encoder, train_preprocess, val_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained=None)\n",
    "\n",
    "setattr(encoder,'val_preprocess',val_preprocess) # trzeba żeby się nie wysypało...\n",
    "setattr(encoder,'train_preprocess',train_preprocess)\n",
    "\n",
    "\n",
    "num_classes = 200\n",
    "embed_dim = encoder.text_projection.shape[-1] if hasattr(encoder, 'text_projection') else encoder.embed_dim\n",
    "dummy_weights = torch.empty(num_classes, embed_dim)\n",
    "\n",
    "head = ClassificationHead(\n",
    "    normalize=True,\n",
    "    weights=dummy_weights\n",
    ")\n",
    "model = ImageClassifier(encoder, head)\n",
    "data = model.load(checkpoint).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789f067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartek/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# IMAGES_PATH = \"../data_new/train\"\n",
    "# class ImageFolderNoSubdirs(Dataset):\n",
    "#     def __init__(self, folder_path :Path, transform=None):\n",
    "#         self.folder_path = folder_path\n",
    "#         self.transform = transform\n",
    "#         self.image_paths: list[Path] = list(Path(folder_path).glob('*.jpeg'))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_path = self.image_paths[idx]\n",
    "#         image = Image.open(image_path)\n",
    "#         image = self.transform(image)\n",
    "#         return image, int(image_path.stem.split(\"_\")[1])\n",
    "\n",
    "# dataset = ImageFolderNoSubdirs(IMAGES_PATH, transform=None)\n",
    "# loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "image_net = ImageNet100(train_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean dataset:   0%|          | 0/407 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean dataset:   0%|          | 0/407 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(batch_images))\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     13\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mappend(labels)\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/zzsn_backdoor_detection/notebooks/../../BadMerging/src/modeling.py:105\u001b[0m, in \u001b[0;36mImageClassifier.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/zzsn_backdoor_detection/notebooks/../../BadMerging/src/modeling.py:100\u001b[0m, in \u001b[0;36mImageClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 100\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head(features)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/open_clip/model.py:416\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    413\u001b[0m         image: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    414\u001b[0m         text: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m ):\n\u001b[0;32m--> 416\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_text(text, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dict:\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/open_clip/model.py:279\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 279\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/open_clip/transformer.py:826\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 826\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m    828\u001b[0m     pooled, tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool(x)\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/open_clip/transformer.py:702\u001b[0m, in \u001b[0;36mVisionTransformer._embeds\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embeds\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 702\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, dim, grid, grid]\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studia/ZZSN/projekt/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "device = \"cuda\"\n",
    "image_net.train_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_images, labels, indexes in tqdm(image_net.train_loader, desc=\"Clean dataset\"):\n",
    "        batch_images = batch_images.to(device)\n",
    "        outputs = model(batch_images)\n",
    "        all_outputs.append(outputs.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# outputs = torch.cat(all_outputs)\n",
    "# labels = torch.cat(all_labels)\n",
    "all_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
